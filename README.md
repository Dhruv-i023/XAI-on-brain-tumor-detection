# XAI-on-brain-tumor-detection
Since the start of the current century, artificial intelligence has gone through critical advances improving the capabilities of intelligent systems. Especially machine learning has changed re- markably and caused the rise of deep learning. Deep learning shows cutting-edge results in terms of even the most advanced, difficult problems. However, that includes a trade-off in terms of in- terpretability. Although traditional machine learning techniques employ interpretable working mechanisms, hybrid systems and deep learning models are black-box being beyond our under- standing capabilities. So, the need for making such systems understandable, additional methods by explainable artificial intelligence (XAI) has been widely developed in last years. In this sense, this study purposes a Convolutional Neural Networks (CNN) model, which is explainded by var- ious xai algorithms such as Grad-CAM, smooth GradCAM and LIME. As providing numerical feedback in addition to the default Grad-CAM, the smooth GradCAM was used within the de- veloped CNN model, in order to have an explainability interface for brain tumor diagnosis. In detail model was evaluated via technical and physicians-oriented (human-side) evaluations. The model provided average findings of 95.00 percent of train accuracy and 99.85 percent of train accuracy.
